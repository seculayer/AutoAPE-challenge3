{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Novel_Author_Classification_AI_Contest.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1PoCUPp5FCaTylxWdLX4lYdYCUaeFg9lt","authorship_tag":"ABX9TyPep0csvWjDRxjbJ3ZV7lfB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TA0E0EZiKxje"},"source":["pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0yezRTZKzHh"},"source":["pip install bert-for-tf2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJFGb2BrKuAs"},"source":["import pandas as pd\n","import re\n","from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.callbacks import  EarlyStopping\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import log_loss\n","from keras.models import load_model\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nM0wfFSiPJPS","executionInfo":{"status":"ok","timestamp":1634181311073,"user_tz":-540,"elapsed":3,"user":{"displayName":"MunAJung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy4ICRmv6jTMkEHJnBIMuHFc78gkSxWStuA68=s64","userId":"09863746772758481190"}},"outputId":"1ced061d-ebf3-4738-8cda-d1577c0937f9"},"source":["cd drive/MyDrive/dacon_psy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/dacon_psy'\n","/content/drive/MyDrive/dacon_psy\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F02AWigHh4W4","executionInfo":{"status":"ok","timestamp":1634181332696,"user_tz":-540,"elapsed":19888,"user":{"displayName":"MunAJung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy4ICRmv6jTMkEHJnBIMuHFc78gkSxWStuA68=s64","userId":"09863746772758481190"}},"outputId":"c8a4f717-a77f-4738-b4dc-8f4e177b765e"},"source":["train = pd.read_csv('train.csv')\n","test = pd.read_csv('test_x.csv')\n","y_train = train['author'].values\n","\n","def cleaning_text(text):\n","    # Clean the text, with the option to remove stopwords and to stem words.\n","    \n","    # Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    # Optionally, remove stop words\n","    \n","    text = \" \".join(text)\n","\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    \n","    text = text.replace('the ','')\n","    text = text.replace('i ','')\n","    \n","    return text\n","\n","def loading(train, test, cleaning_text):\n","  train_sentences = list(train['text'])\n","  train_texts = []\n","  for sen in train_sentences:\n","    train_texts.append(cleaning_text(sen))\n","  print('cleaning train texts completed')\n","  del train_sentences, train, sen\n","  test_sentences = list(test['text'])\n","  test_texts = []\n","  for sen in test_sentences:\n","    test_texts.append(cleaning_text(sen))\n","  print('cleaning test texts completed')\n","\n","  with open('author.txt','w',encoding='utf-8') as f:\n","      f.write('\\n'.join(train_texts))\n","  del f\n","\n","  SentencePieceTrainer.Train('--input=author.txt --model_prefix=author --vocab_size=5000', max_df = 0.80, min_df = 0.25)\n","\n","  sp = SentencePieceProcessor()\n","  sp.Load(\"author.model\")\n","\n","  cv = CountVectorizer(tokenizer = sp.encode_as_pieces)\n","\n","  train_pad = cv.fit_transform(train_texts).toarray()\n","  del train_texts\n","  test_pad = cv.transform(test_texts).toarray()\n","  del test_texts, sp, cv\n","  print('data setting completed')\n","  del test_sentences, test, sen, cleaning_text\n","  return train_pad, test_pad\n","\n","train_pad, test_pad = loading(train, test, cleaning_text)\n","print(train_pad.shape)\n","del SentencePieceProcessor, SentencePieceTrainer, CountVectorizer"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cleaning train texts completed\n","cleaning test texts completed\n","data setting completed\n","(54879, 5142)\n"]}]},{"cell_type":"code","metadata":{"id":"2FUE9gliKqLD"},"source":["def create_dnn(input_shape):\n","  input = layers.Input(shape=(input_shape,), name='INPUT')\n","  dnn = layers.Dense(1024)(input)\n","  batch = layers.BatchNormalization()(dnn)\n","  relu = layers.ReLU()(batch)\n","  drop = layers.Dropout(0.15)(relu)\n","  dnn = layers.Dense(256)(drop)\n","  batch = layers.BatchNormalization()(dnn)\n","  relu = layers.ReLU()(batch)\n","  drop = layers.Dropout(0.15)(relu)\n","  dnn = layers.Dense(64)(drop)\n","  batch = layers.BatchNormalization()(dnn)\n","  relu = layers.ReLU()(batch)\n","  drop = layers.Dropout(0.30)(relu)\n","  dnn = layers.Dense(16)(drop)\n","  batch = layers.BatchNormalization()(dnn)\n","  relu = layers.ReLU()(batch)\n","  drop = layers.Dropout(0.30)(relu)\n","  last_layer = layers.Dense(5, activation = 'softmax')(drop)\n","\n","  model = models.Model(inputs=input, outputs=last_layer)\n","  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.Adam(0.0005))\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2-UxmlAK36M"},"source":["model = create_dnn(train_pad.shape[1])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPSzqpZ2K4rj"},"source":["def creating_results(seed, train_pad, test_pad, y_train, fold):\n","  batch_size =  32\n","  epochs = 300\n","  predicts = []\n","  scores = []\n","  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","  es = EarlyStopping(monitor='val_loss', verbose=0, patience=1)\n","  fold_2 = 0\n","  for train_index, val_index in skf.split(train_pad, y_train):\n","    path = 'submissions/DNN_'+str(fold)+'_'+str(fold_2)\n","    filepath_val_loss=\"models/\"+path+\".tf\"\n","    checkpoint_val_loss = ModelCheckpoint(filepath_val_loss, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n","    model = create_dnn(train_pad.shape[1])\n","    tr_X = train_pad[train_index]\n","    tr_y = y_train[train_index]\n","\n","    val_X = train_pad[val_index]\n","    val_y = y_train[val_index]\n","\n","    model.fit(tr_X,tr_y,\n","                        batch_size=batch_size,\n","                        epochs=epochs, \n","                        validation_data=(val_X,val_y), \n","                        shuffle=True, \n","                        verbose=0,\n","                        callbacks=[es,checkpoint_val_loss] #LearningRateScheduler(lr_decay)\n","                        ) \n","    \n","    model = load_model(filepath_val_loss)\n","    predicts.append(model.predict(test_pad))\n","    score = log_loss(val_y,model.predict(val_X))\n","    scores.append(score)\n","    fold_2 += 1\n","    del tr_X,tr_y,val_X,val_y,model,score\n","  prediction = pd.DataFrame(np.mean(predicts,axis=0)).reset_index()\n","  score = np.mean(scores)\n","  print(score)\n","  file_path = 'submissions/DNN_'+str(score)[:8]+'.csv'\n","  prediction.to_csv(file_path, index=False) \n","  print('#'*25,\"FINISHED  : \",score, ' '*10,'#'*25)\n","  del prediction, score, file_path, filepath_val_loss,es,skf,train_index,val_index,scores,predicts,epochs,batch_size,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ENkTBaVLMKa"},"source":["fold = 0\n","for seed in [1210*x for x in range(5)]:\n","  fold += 1\n","  creating_results(seed, train_pad, test_pad, y_train, fold)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hemGWXEHLOoV","executionInfo":{"status":"ok","timestamp":1634171226265,"user_tz":-540,"elapsed":6051,"user":{"displayName":"MunAJung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy4ICRmv6jTMkEHJnBIMuHFc78gkSxWStuA68=s64","userId":"09863746772758481190"}},"outputId":"6eef809b-4a17-4049-c948-061efe920d05"},"source":["import pandas as pd\n","\n","train = pd.read_csv('train.csv')\n","test = pd.read_csv('test_x.csv')\n","y_train = train['author'].values\n","\n","del pd\n","\n","def cleaning_text(text):\n","    # Clean the text, with the option to remove stopwords and to stem words.\n","    \n","    # Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    # Optionally, remove stop words\n","    \n","    text = \" \".join(text)\n","\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    \n","    text = text.replace('the ','')\n","    text = text.replace('i ','')\n","    \n","    return text\n","\n","import re\n","train_sentences = list(train['text'])\n","train_texts = []\n","for sen in train_sentences:\n","  train_texts.append(cleaning_text(sen))\n","print('cleaning train texts completed')\n","del train_sentences, train, sen\n","test_sentences = list(test['text'])\n","test_texts = []\n","for sen in test_sentences:\n","  test_texts.append(cleaning_text(sen))\n","print('cleaning test texts completed')\n","del test_sentences, test, sen, cleaning_text, re"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cleaning train texts completed\n","cleaning test texts completed\n"]}]},{"cell_type":"code","metadata":{"id":"flc_9YMLLUKk"},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import bert\n","\n","BertTokenizer = bert.bert_tokenization.FullTokenizer\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                            trainable=False)\n","vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = BertTokenizer(vocabulary_file, to_lower_case)       \n","\n","VOCAB_LENGTH = len(tokenizer.vocab)\n","\n","def tokenize_text(text):\n","    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n","\n","train_token = [tokenize_text(text) for text in train_texts]\n","test_token = [tokenize_text(text) for text in test_texts]\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","texts_with_len = [[text, len(text)]\n","                 for i, text in enumerate(train_token)]\n","texts_with_len.sort(key=lambda x: x[1], reverse=True)\n","max_len = texts_with_len[0][1]\n","k = 0\n","for _,length in texts_with_len:\n","  if length == 0:\n","    k+= 1\n","texts_with_len = texts_with_len[k:]\n","padding_type = 'post'\n","train_pad = pad_sequences(train_token, padding=padding_type, maxlen=max_len)\n","test_pad = pad_sequences(test_token, padding=padding_type, maxlen=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaegQbURLWO0","executionInfo":{"status":"ok","timestamp":1634171312857,"user_tz":-540,"elapsed":12507,"user":{"displayName":"MunAJung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy4ICRmv6jTMkEHJnBIMuHFc78gkSxWStuA68=s64","userId":"09863746772758481190"}},"outputId":"a5fbc592-435d-4107-c03b-b4188ba56ccf"},"source":["import numpy as np\n","import pandas as pd\n","train_df = pd.DataFrame(train_pad)\n","test_df = pd.DataFrame(test_pad)\n","print('train의 shape', train_df.shape)\n","print('test의 shape', test_df.shape)\n","print('train 중 0으로만 이루어져 있는 문장 중 max : ', np.sum(train_df == 0, axis=1).max())\n","print('test 중 0으로만 이루어져 있는 문장 중 max : ', np.sum(test_df == 0, axis=1).max())\n","\n","print('문장이라고 인식되기 위해서는 최소 5단어 이상이라고 판단')\n","drop_index = train_df.loc[np.sum(train_df == 0, axis=1) > np.sum(train_df == 0, axis=1).max()-5].index\n","train_df = train_df.drop(index=drop_index)\n","y_train = pd.DataFrame(y_train).drop(index=drop_index)\n","print('##########너무 적은 단어 수 제거##########')\n","print('train 중 0으로만 이루어져 있는 문장 중 max :', np.sum(train_df == 0, axis=1).max())\n","print('test 중 0으로만 이루어져 있는 문장 중 max : ', np.sum(test_df == 0, axis=1).max())\n","\n","train_word = train_df.applymap(lambda x : 1 if x > 0 else 0)\n","test_word = test_df.applymap(lambda x : 1 if x > 0 else 0)\n","\n","print('\\ntrain_word 중 하나의 문장으로만 이루어진 문장 시작점 : ',train_word.loc[:,(train_word.sum()==1)].columns[0])\n","print('test_word 중 하나의 문장으로만 이루어진 문장 시작점 : ',test_word.loc[:,(test_word.sum()==1)].columns[0])\n","print('train_word 중 하나의 문장으로만 이루어진 문장 끝나는 지점 : ',train_word.loc[:,(train_word.sum()==1)].columns[-1])\n","print('test_word 중 하나의 문장으로만 이루어진 문장 끝나는 지점 : ',test_word.loc[:,(test_word.sum()==1)].columns[-1])\n","\n","print('*test의 처리를 위해서 따로 처리는 하지 않는다.*')\n","\n","print('\\ntrain_word 중 두개의 문장으로만 이루어진 문장 시작점 : ',train_word.loc[:,(train_word.sum()==2)].columns[0])\n","print('test_word 중 두개의 문장으로만 이루어진 문장 시작점 : ',test_word.loc[:,(test_word.sum()==2)].columns[0])\n","print('train_word 중 두개의 문장으로만 이루어진 문장 끝나는 지점 : ',train_word.loc[:,(train_word.sum()==2)].columns[-1])\n","print('test_word 중 두개의 문장으로만 이루어진 문장 끝나는 지점 : ',test_word.loc[:,(test_word.sum()==2)].columns[-1])\n","\n","print('*test의 처리를 위해서 따로 처리는 하지 않는다.*')\n","print('\\n최종 train shape : ',train_df.shape)\n","print('최종 test shape : ', test_df.shape)\n","print('최종 y_train shape :', y_train.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train의 shape (54879, 476)\n","test의 shape (19617, 476)\n","train 중 0으로만 이루어져 있는 문장 중 max :  476\n","test 중 0으로만 이루어져 있는 문장 중 max :  451\n","문장이라고 인식되기 위해서는 최소 5단어 이상이라고 판단\n","##########너무 적은 단어 수 제거##########\n","train 중 0으로만 이루어져 있는 문장 중 max : 471\n","test 중 0으로만 이루어져 있는 문장 중 max :  451\n","\n","train_word 중 하나의 문장으로만 이루어진 문장 시작점 :  429\n","test_word 중 하나의 문장으로만 이루어진 문장 시작점 :  451\n","train_word 중 하나의 문장으로만 이루어진 문장 끝나는 지점 :  475\n","test_word 중 하나의 문장으로만 이루어진 문장 끝나는 지점 :  460\n","*test의 처리를 위해서 따로 처리는 하지 않는다.*\n","\n","train_word 중 두개의 문장으로만 이루어진 문장 시작점 :  423\n","test_word 중 두개의 문장으로만 이루어진 문장 시작점 :  421\n","train_word 중 두개의 문장으로만 이루어진 문장 끝나는 지점 :  428\n","test_word 중 두개의 문장으로만 이루어진 문장 끝나는 지점 :  450\n","*test의 처리를 위해서 따로 처리는 하지 않는다.*\n","\n","최종 train shape :  (54105, 476)\n","최종 test shape :  (19617, 476)\n","최종 y_train shape : (54105, 1)\n"]}]},{"cell_type":"code","metadata":{"id":"_hE_OW0fLb9S"},"source":["train_pad = np.array(train_df)\n","test_pad = np.array(test_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tYPSZ--LeAA"},"source":["import numpy as np\n","np.save('train_pad', arr = train_pad)\n","np.save('test_pad', arr = test_pad)\n","np.save('y_train', arr=y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWxqTXpLLe3V"},"source":["from tensorflow.keras import models, layers, optimizers\n","from tensorflow.keras.callbacks import  EarlyStopping\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.models import load_model\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import log_loss\n","\n","train_pad = np.load('train_pad.npy')\n","test_pad = np.load('test_pad.npy')\n","y_train = np.load('y_train.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2cVVAY5LjBM"},"source":["def CNN_1234KS(VOCAB_LENGTH, EMB_DIM, input_shape):\n","  input = layers.Input(shape=(input_shape,), name='INPUT')\n","  emb = layers.Embedding(VOCAB_LENGTH, EMB_DIM, name='EMBEDDING')(input)\n","  first = layers.Conv1D(128, kernel_size = 1, padding='same', activation='relu', name = 'CNN_first_1')(emb)\n","  batch_1 = layers.BatchNormalization(name = 'CNN_first_batch')(first)\n","  pool_1 = layers.GlobalMaxPooling1D(name = 'CNN_first_pooling')(batch_1)\n","  second = layers.Conv1D(128, kernel_size = 2, padding='same', activation='relu', name = 'CNN_second_1')(emb)\n","  batch_2 = layers.BatchNormalization(name = 'CNN_second_batch')(second)\n","  pool_2 = layers.GlobalMaxPooling1D(name = 'CNN_second_pooling')(batch_2)\n","  third = layers.Conv1D(128, kernel_size = 3, padding='same', activation='relu', name = 'CNN_third_1')(emb)\n","  batch_3 = layers.BatchNormalization(name = 'CNN_third_batch')(third)\n","  pool_3 = layers.GlobalMaxPooling1D(name = 'CNN_third_pooling')(batch_3)\n","  four = layers.Conv1D(128, kernel_size = 4, padding='same', activation='relu', name = 'CNN_fourth_1')(emb)\n","  batch_4 = layers.BatchNormalization(name = 'CNN_fourth_batch')(four)\n","  pool_4 = layers.GlobalMaxPooling1D(name = 'CNN_fourth_pooling')(batch_4)\n","  Den_1 = layers.Dense(128, name = 'input_Dense_1')(input)\n","  Den_batch = layers.BatchNormalization(name = 'input_Dense_batch')(Den_1)\n","  concated_1D = layers.concatenate([pool_1,pool_2,pool_3, pool_4, Den_batch], name ='CONCAT')\n","  dropout_15 = layers.Dropout(0.15, name='FC_dropout_1')(concated_1D)\n","  FC = layers.Dense(128, activation = 'relu', name = 'FC_Dense_1')(dropout_15)\n","  dropout_15 = layers.Dropout(0.15 ,name = 'FC_dropout_2')(FC)\n","  FC = layers.Dense(32, activation = 'relu',name = 'FC_Dense_2')(dropout_15)\n","  dropout_30 = layers.Dropout(0.30, name='FC_dropout_3')(FC)\n","  last_layer = layers.Dense(5, activation='softmax', name='Final_Layer')(dropout_30)\n","  model = models.Model(inputs=input, outputs=last_layer)\n","  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.Adam())\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bd5CK2dHLja8"},"source":["def CNN_234KS(VOCAB_LENGTH, EMB_DIM, input_shape):\n","\n","  input = layers.Input(shape=(input_shape,), name='INPUT')\n","  emb = layers.Embedding(VOCAB_LENGTH, EMB_DIM, name='EMBEDDING')(input)\n","  first = layers.Conv1D(128, kernel_size = 2, padding='same',  name = 'CNN_first_1')(emb)\n","  batch_1 = layers.BatchNormalization(name = 'CNN_first_batch')(first)\n","  lelu_1 = layers.LeakyReLU()(batch_1)\n","  pool_1 = layers.GlobalMaxPooling1D(name = 'CNN_first_pooling')(lelu_1)\n","\n","  second = layers.Conv1D(128, kernel_size = 3, padding='same', name = 'CNN_second_1')(emb)\n","  batch_2 = layers.BatchNormalization(name = 'CNN_second_batch')(second)\n","  lelu_2 = layers.LeakyReLU()(batch_2)\n","  pool_2 = layers.GlobalMaxPooling1D(name = 'CNN_second_pooling')(batch_2)\n","\n","  third = layers.Conv1D(128, kernel_size = 4, padding='same', name = 'CNN_third_1')(emb)\n","  batch_3 = layers.BatchNormalization(name = 'CNN_third_batch')(third)\n","  lelu_3 = layers.LeakyReLU()(batch_3)\n","  pool_3 = layers.GlobalMaxPooling1D(name = 'CNN_third_pooling')(batch_3)\n","\n","  concated_1D = layers.concatenate([pool_1,pool_2,pool_3], name ='CONCAT')\n","\n","  dropout_15 = layers.Dropout(0.30, name='FC_dropout_1')(concated_1D)\n","  FC = layers.Dense(128, name = 'FC_Dense_1')(dropout_15)\n","  batch = layers.BatchNormalization()(FC)\n","  relu = layers.ReLU()(batch)\n","  dropout_15 = layers.Dropout(0.15 ,name = 'FC_dropout_2')(relu)\n","  FC = layers.Dense(32,name = 'FC_Dense_2')(dropout_15)\n","  batch = layers.BatchNormalization()(FC)\n","  relu = layers.ReLU()(batch)\n","  dropout_30 = layers.Dropout(0.30, name='FC_dropout_3')(relu)\n","\n","  last_layer = layers.Dense(5, activation='softmax', name='Final_Layer')(dropout_30)\n","\n","  model = models.Model(inputs=input, outputs=last_layer)\n","  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.Adam())\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTqCGtlULkso"},"source":["def CNN_NO_OPTI(VOCAB_LENGTH, EMB_DIM, input_shape):\n","\n","  input = layers.Input(shape=(input_shape,), name='INPUT')\n","  emb = layers.Embedding(VOCAB_LENGTH, EMB_DIM, name='EMBEDDING')(input)\n","  first = layers.Conv1D(128, kernel_size = 2, padding='same',  name = 'CNN_first_1')(emb)\n","  batch_1 = layers.BatchNormalization(name = 'CNN_first_batch')(first)\n","  pool_1 = layers.GlobalMaxPooling1D(name = 'CNN_first_pooling')(batch_1)\n","\n","  second = layers.Conv1D(128, kernel_size = 3, padding='same', name = 'CNN_second_1')(emb)\n","  batch_2 = layers.BatchNormalization(name = 'CNN_second_batch')(second)\n","  pool_2 = layers.GlobalMaxPooling1D(name = 'CNN_second_pooling')(batch_2)\n","\n","  third = layers.Conv1D(128, kernel_size = 4, padding='same', name = 'CNN_third_1')(emb)\n","  batch_3 = layers.BatchNormalization(name = 'CNN_third_batch')(third)\n","  pool_3 = layers.GlobalMaxPooling1D(name = 'CNN_third_pooling')(batch_3)\n","\n","  concated_1D = layers.concatenate([pool_1,pool_2,pool_3], name ='CONCAT')\n","\n","  dropout_15 = layers.Dropout(0.30, name='FC_dropout_1')(concated_1D)\n","  FC = layers.Dense(128, name = 'FC_Dense_1')(dropout_15)\n","  batch = layers.BatchNormalization()(FC)\n","  relu = layers.ReLU()(batch)\n","  dropout_15 = layers.Dropout(0.15 ,name = 'FC_dropout_2')(relu)\n","  FC = layers.Dense(32,name = 'FC_Dense_2')(dropout_15)\n","  batch = layers.BatchNormalization()(FC)\n","  relu = layers.ReLU()(batch)\n","  dropout_30 = layers.Dropout(0.30, name='FC_dropout_3')(relu)\n","\n","  last_layer = layers.Dense(5, activation='softmax', name='Final_Layer')(dropout_30)\n","\n","  model = models.Model(inputs=input, outputs=last_layer)\n","  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.Adam())\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHj6r09sLm35"},"source":["def creating_results(seed, train_pad, test_pad, y_train, fold, model, model_name):\n","  batch_size =  32\n","  epochs = 300\n","  predicts = []\n","  scores = []\n","  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","  es = EarlyStopping(monitor='val_loss', verbose=0, patience=1)\n","  fold_2 = 0\n","  for train_index, val_index in skf.split(train_pad, y_train):\n","    path = 'submissions/'+model_name+'_'+str(fold)+'_'+str(fold_2)\n","    filepath_val_loss=\"models/\"+path+\".tf\"\n","    checkpoint_val_loss = ModelCheckpoint(filepath_val_loss, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n","    tr_X = train_pad[train_index]\n","    tr_y = y_train[train_index]\n","\n","    val_X = train_pad[val_index]\n","    val_y = y_train[val_index]\n","\n","    model.fit(tr_X,tr_y,\n","                        batch_size=batch_size,\n","                        epochs=epochs, \n","                        validation_data=(val_X,val_y), \n","                        shuffle=True, \n","                        verbose=0,\n","                        callbacks=[es,checkpoint_val_loss] #LearningRateScheduler(lr_decay)\n","                        ) \n","    \n","    model = load_model(filepath_val_loss)\n","    predicts.append(model.predict(test_pad))\n","    score = log_loss(val_y,model.predict(val_X))\n","    scores.append(score)\n","    fold_2 += 1\n","  prediction = pd.DataFrame(np.mean(predicts,axis=0)).reset_index()\n","  score = np.mean(scores)\n","  print(score)\n","  file_path = 'submissions/'+model_name+'_'+str(score)[:8]+'.csv'\n","  prediction.to_csv(file_path, index=False) \n","  print('#'*25,\"FINISHED  : \",score, ' '*10,'#'*25)\n","  del prediction, score, file_path, filepath_val_loss,es,skf,train_index,val_index,scores,predicts,epochs,batch_size,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iMkFKXTLvu9"},"source":["EMB_DIM = 256\n","for seed in [1530*x for x in range(7)]:\n","  model = CNN_1234KS(VOCAB_LENGTH, EMB_DIM, train_pad.shape[1])\n","  model_name = 'CNN_1234KS'\n","  fold += 1\n","  creating_results(seed, train_pad, test_pad, y_train, fold, model, model_name)\n","for seed in [1530*x for x in range(7)]:\n","  model = CNN_234KS(VOCAB_LENGTH, EMB_DIM, train_pad.shape[1])\n","  model_name = 'CNN_234KS'\n","  fold += 1\n","  creating_results(seed, train_pad, test_pad, y_train, fold, model, model_name)\n","for seed in [1530*x for x in range(7)]:\n","  model = CNN_NO_OPTI(VOCAB_LENGTH, EMB_DIM, train_pad.shape[1])\n","  model_name = 'CNN_NO_OPTI'\n","  fold += 1\n","  creating_results(seed, train_pad, test_pad, y_train, fold, model, model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqRBUfXGLwxx"},"source":["from scipy.stats import gmean\n","import glob\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import matthews_corrcoef"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_s_GTdtLyas"},"source":["pred_list = glob.glob('submissions/*DNN*.csv')[:5]\n","\n","sub1 = pd.read_csv(pred_list[0]).iloc[:,1:]\n","sub2 = pd.read_csv(pred_list[1]).iloc[:,1:]\n","sub3 = pd.read_csv(pred_list[2]).iloc[:,1:]\n","sub4 = pd.read_csv(pred_list[3]).iloc[:,1:]\n","sub5 = pd.read_csv(pred_list[4]).iloc[:,1:]\n","\n","axis_0 = gmean([sub1.loc[:,'0'],sub2.loc[:,'0'],sub3.loc[:,'0'],sub4.loc[:,'0'],sub5.loc[:,'0']],axis=0)\n","axis_1 = gmean([sub1.loc[:,'1'],sub2.loc[:,'1'],sub3.loc[:,'1'],sub4.loc[:,'1'],sub5.loc[:,'1']],axis=0)\n","axis_2 = gmean([sub1.loc[:,'2'],sub2.loc[:,'2'],sub3.loc[:,'2'],sub4.loc[:,'2'],sub5.loc[:,'2']],axis=0)\n","axis_3 = gmean([sub1.loc[:,'3'],sub2.loc[:,'3'],sub3.loc[:,'3'],sub4.loc[:,'3'],sub5.loc[:,'3']],axis=0)\n","axis_4 = gmean([sub1.loc[:,'4'],sub2.loc[:,'4'],sub3.loc[:,'4'],sub4.loc[:,'4'],sub5.loc[:,'4']],axis=0)\n","\n","DNN = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T\n","\n","pred_list = glob.glob('submissions/*CNN_1234KS*.csv')[:5]\n","\n","sub1 = pd.read_csv(pred_list[0]).iloc[:,1:]\n","sub2 = pd.read_csv(pred_list[1]).iloc[:,1:]\n","sub3 = pd.read_csv(pred_list[2]).iloc[:,1:]\n","sub4 = pd.read_csv(pred_list[3]).iloc[:,1:]\n","sub5 = pd.read_csv(pred_list[4]).iloc[:,1:]\n","\n","axis_0 = gmean([sub1.loc[:,'0'],sub2.loc[:,'0'],sub3.loc[:,'0'],sub4.loc[:,'0'],sub5.loc[:,'0']],axis=0)\n","axis_1 = gmean([sub1.loc[:,'1'],sub2.loc[:,'1'],sub3.loc[:,'1'],sub4.loc[:,'1'],sub5.loc[:,'1']],axis=0)\n","axis_2 = gmean([sub1.loc[:,'2'],sub2.loc[:,'2'],sub3.loc[:,'2'],sub4.loc[:,'2'],sub5.loc[:,'2']],axis=0)\n","axis_3 = gmean([sub1.loc[:,'3'],sub2.loc[:,'3'],sub3.loc[:,'3'],sub4.loc[:,'3'],sub5.loc[:,'3']],axis=0)\n","axis_4 = gmean([sub1.loc[:,'4'],sub2.loc[:,'4'],sub3.loc[:,'4'],sub4.loc[:,'4'],sub5.loc[:,'4']],axis=0)\n","\n","CNN_1234KS = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T\n","\n","pred_list = glob.glob('submissions/*CNN_234KS*.csv')[:5]\n","\n","sub1 = pd.read_csv(pred_list[0]).iloc[:,1:]\n","sub2 = pd.read_csv(pred_list[1]).iloc[:,1:]\n","sub3 = pd.read_csv(pred_list[2]).iloc[:,1:]\n","sub4 = pd.read_csv(pred_list[3]).iloc[:,1:]\n","sub5 = pd.read_csv(pred_list[4]).iloc[:,1:]\n","\n","axis_0 = gmean([sub1.loc[:,'0'],sub2.loc[:,'0'],sub3.loc[:,'0'],sub4.loc[:,'0'],sub5.loc[:,'0']],axis=0)\n","axis_1 = gmean([sub1.loc[:,'1'],sub2.loc[:,'1'],sub3.loc[:,'1'],sub4.loc[:,'1'],sub5.loc[:,'1']],axis=0)\n","axis_2 = gmean([sub1.loc[:,'2'],sub2.loc[:,'2'],sub3.loc[:,'2'],sub4.loc[:,'2'],sub5.loc[:,'2']],axis=0)\n","axis_3 = gmean([sub1.loc[:,'3'],sub2.loc[:,'3'],sub3.loc[:,'3'],sub4.loc[:,'3'],sub5.loc[:,'3']],axis=0)\n","axis_4 = gmean([sub1.loc[:,'4'],sub2.loc[:,'4'],sub3.loc[:,'4'],sub4.loc[:,'4'],sub5.loc[:,'4']],axis=0)\n","\n","CNN_234KS = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T\n","\n","pred_list = glob.glob('submissions/*CNN_NO_OPTI*.csv')[:5]\n","\n","sub1 = pd.read_csv(pred_list[0]).iloc[:,1:]\n","sub2 = pd.read_csv(pred_list[1]).iloc[:,1:]\n","sub3 = pd.read_csv(pred_list[2]).iloc[:,1:]\n","sub4 = pd.read_csv(pred_list[3]).iloc[:,1:]\n","sub5 = pd.read_csv(pred_list[4]).iloc[:,1:]\n","\n","axis_0 = gmean([sub1.loc[:,'0'],sub2.loc[:,'0'],sub3.loc[:,'0'],sub4.loc[:,'0'],sub5.loc[:,'0']],axis=0)\n","axis_1 = gmean([sub1.loc[:,'1'],sub2.loc[:,'1'],sub3.loc[:,'1'],sub4.loc[:,'1'],sub5.loc[:,'1']],axis=0)\n","axis_2 = gmean([sub1.loc[:,'2'],sub2.loc[:,'2'],sub3.loc[:,'2'],sub4.loc[:,'2'],sub5.loc[:,'2']],axis=0)\n","axis_3 = gmean([sub1.loc[:,'3'],sub2.loc[:,'3'],sub3.loc[:,'3'],sub4.loc[:,'3'],sub5.loc[:,'3']],axis=0)\n","axis_4 = gmean([sub1.loc[:,'4'],sub2.loc[:,'4'],sub3.loc[:,'4'],sub4.loc[:,'4'],sub5.loc[:,'4']],axis=0)\n","\n","CNN_NO_OPTI = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpojC9SaL2Uq"},"source":["axis_0 = gmean([DNN.loc[:,0],CNN_1234KS.loc[:,0]],axis=0)\n","axis_1 = gmean([DNN.loc[:,1],CNN_1234KS.loc[:,1]],axis=0)\n","axis_2 = gmean([DNN.loc[:,2],CNN_1234KS.loc[:,2]],axis=0)\n","axis_3 = gmean([DNN.loc[:,3],CNN_1234KS.loc[:,3]],axis=0)\n","axis_4 = gmean([DNN.loc[:,4],CNN_1234KS.loc[:,4]],axis=0)\n","\n","first_layer = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T\n","\n","axis_0 = gmean([first_layer.loc[:,0],CNN_234KS.loc[:,0]],axis=0)\n","axis_1 = gmean([first_layer.loc[:,1],CNN_234KS.loc[:,1]],axis=0)\n","axis_2 = gmean([first_layer.loc[:,2],CNN_234KS.loc[:,2]],axis=0)\n","axis_3 = gmean([first_layer.loc[:,3],CNN_234KS.loc[:,3]],axis=0)\n","axis_4 = gmean([first_layer.loc[:,4],CNN_234KS.loc[:,4]],axis=0)\n","\n","second_layer = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T\n","\n","axis_0 = gmean([second_layer.loc[:,0],CNN_NO_OPTI.loc[:,0]],axis=0)\n","axis_1 = gmean([second_layer.loc[:,1],CNN_NO_OPTI.loc[:,1]],axis=0)\n","axis_2 = gmean([second_layer.loc[:,2],CNN_NO_OPTI.loc[:,2]],axis=0)\n","axis_3 = gmean([second_layer.loc[:,3],CNN_NO_OPTI.loc[:,3]],axis=0)\n","axis_4 = gmean([second_layer.loc[:,4],CNN_NO_OPTI.loc[:,4]],axis=0)\n","\n","third_layer = pd.DataFrame([axis_0,axis_1,axis_2,axis_3,axis_4]).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ep1w-D4EL5DZ","executionInfo":{"status":"ok","timestamp":1634188951584,"user_tz":-540,"elapsed":3464,"user":{"displayName":"MunAJung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy4ICRmv6jTMkEHJnBIMuHFc78gkSxWStuA68=s64","userId":"09863746772758481190"}},"outputId":"5539ad31-4fa0-42c0-c240-c75430327dd6"},"source":["pred_list = [DNN, CNN_1234KS, CNN_234KS, CNN_NO_OPTI]\n","corrs = []\n","for pred in pred_list:\n","    corrs.append(pred.apply(lambda x : np.argmax([x[0],x[1],x[2],x[3],x[4]]),axis=1).values)\n","print(np.corrcoef(corrs))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.82534635 0.83085312 0.834004  ]\n"," [0.82534635 1.         0.92990904 0.93265261]\n"," [0.83085312 0.92990904 1.         0.95073555]\n"," [0.834004   0.93265261 0.95073555 1.        ]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITwN-f_6L6GL","executionInfo":{"status":"ok","timestamp":1634188953059,"user_tz":-540,"elapsed":1484,"user":{"displayName":"MunAJung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy4ICRmv6jTMkEHJnBIMuHFc78gkSxWStuA68=s64","userId":"09863746772758481190"}},"outputId":"53a05698-fbc0-4fbd-9a04-11d7acc84cf7"},"source":["pred_list = [first_layer, second_layer, third_layer]\n","corrs = []\n","for pred in pred_list:\n","    corrs.append(pred.apply(lambda x : np.argmax([x[0],x[1],x[2],x[3],x[4]]),axis=1).values)\n","print(np.corrcoef(corrs))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.95314418 0.93715232]\n"," [0.95314418 1.         0.97217347]\n"," [0.93715232 0.97217347 1.        ]]\n"]}]},{"cell_type":"code","metadata":{"id":"1f0wOcyAL7Hu"},"source":["third_layer.reset_index().to_csv('final_submission_3.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qm4kfsrlI3Sa"},"source":["  "],"execution_count":null,"outputs":[]}]}