{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Emwmw_mWwzG",
        "outputId": "a27e8253-6b0d-4e84-b40e-4f7fc4bfdf3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Iagvy__XWzzD",
        "outputId": "61cc9686-5258-4e02-8451-7788748d8f96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a7c18d8-3303-42e2-ab82-86cd2925d65c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a7c18d8-3303-42e2-ab82-86cd2925d65c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a7c18d8-3303-42e2-ab82-86cd2925d65c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a7c18d8-3303-42e2-ab82-86cd2925d65c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk # the natural langauage toolkit, open-source NLP\n",
        "import pandas as pd\n",
        "traindata = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/train.csv\")\n",
        "traindata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4jzO_gEXReZ",
        "outputId": "a503aa50-16e4-4165-b12a-4f6dd0e8184d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ConditionalFreqDist with 3 conditions>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#word frequency\n",
        "\n",
        "byAuthor = traindata.groupby(\"author\")\n",
        "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
        "for name, group in byAuthor:\n",
        "    # get all of the sentences they wrote and collapse them into a\n",
        "    # single long string\n",
        "    sentences = group['text'].str.cat(sep = ' ')\n",
        "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
        "    # the same word rather than two different words)\n",
        "    sentences = sentences.lower()\n",
        "    # split the text into individual tokens    \n",
        "    nltk.download('punkt')\n",
        "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
        "    # calculate the frequency of each token\n",
        "    frequency = nltk.FreqDist(tokens)\n",
        "    # add the frequencies for each author to our dictionary\n",
        "    wordFreqByAuthor[name] = (frequency)\n",
        "wordFreqByAuthor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r32CdDiH-91d",
        "outputId": "9dee6858-6746-461a-f0b0-be3b701a25a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 / 8392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 / 8392\n",
            "300 / 8392\n",
            "400 / 8392\n",
            "500 / 8392\n",
            "600 / 8392\n",
            "700 / 8392\n",
            "800 / 8392\n",
            "900 / 8392\n",
            "1000 / 8392\n",
            "1100 / 8392\n",
            "1200 / 8392\n",
            "1300 / 8392\n",
            "1400 / 8392\n",
            "1500 / 8392\n",
            "1600 / 8392\n",
            "1700 / 8392\n",
            "1800 / 8392\n",
            "1900 / 8392\n",
            "2000 / 8392\n",
            "2100 / 8392\n",
            "2200 / 8392\n",
            "2300 / 8392\n",
            "2400 / 8392\n",
            "2500 / 8392\n",
            "2600 / 8392\n",
            "2700 / 8392\n",
            "2800 / 8392\n",
            "2900 / 8392\n",
            "3000 / 8392\n",
            "3100 / 8392\n",
            "3200 / 8392\n",
            "3300 / 8392\n",
            "3400 / 8392\n",
            "3500 / 8392\n",
            "3600 / 8392\n",
            "3700 / 8392\n",
            "3800 / 8392\n",
            "3900 / 8392\n",
            "4000 / 8392\n",
            "4100 / 8392\n",
            "4200 / 8392\n",
            "4300 / 8392\n",
            "4400 / 8392\n",
            "4500 / 8392\n",
            "4600 / 8392\n",
            "4700 / 8392\n",
            "4800 / 8392\n",
            "4900 / 8392\n",
            "5000 / 8392\n",
            "5100 / 8392\n",
            "5200 / 8392\n",
            "5300 / 8392\n",
            "5400 / 8392\n",
            "5500 / 8392\n",
            "5600 / 8392\n",
            "5700 / 8392\n",
            "5800 / 8392\n",
            "5900 / 8392\n",
            "6000 / 8392\n",
            "6100 / 8392\n",
            "6200 / 8392\n",
            "6300 / 8392\n",
            "6400 / 8392\n",
            "6500 / 8392\n",
            "6600 / 8392\n",
            "6700 / 8392\n",
            "6800 / 8392\n",
            "6900 / 8392\n",
            "7000 / 8392\n",
            "7100 / 8392\n",
            "7200 / 8392\n",
            "7300 / 8392\n",
            "7400 / 8392\n",
            "7500 / 8392\n",
            "7600 / 8392\n",
            "7700 / 8392\n",
            "7800 / 8392\n",
            "7900 / 8392\n",
            "8000 / 8392\n",
            "8100 / 8392\n",
            "8200 / 8392\n",
            "8300 / 8392\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        EAP           HPL           MWS\n",
              "0  0.000158  1.245376e-05  9.998294e-01\n",
              "1  1.000000  3.284651e-08  2.224809e-10\n",
              "2  0.002119  9.978809e-01  2.397010e-08\n",
              "3  0.201367  7.986332e-01  8.969638e-10\n",
              "4  0.965843  9.432286e-03  2.472491e-02"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-189b0cb0-c37c-4aac-bf25-0f1cac1b0f26\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EAP</th>\n",
              "      <th>HPL</th>\n",
              "      <th>MWS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000158</td>\n",
              "      <td>1.245376e-05</td>\n",
              "      <td>9.998294e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.284651e-08</td>\n",
              "      <td>2.224809e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002119</td>\n",
              "      <td>9.978809e-01</td>\n",
              "      <td>2.397010e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.201367</td>\n",
              "      <td>7.986332e-01</td>\n",
              "      <td>8.969638e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.965843</td>\n",
              "      <td>9.432286e-03</td>\n",
              "      <td>2.472491e-02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-189b0cb0-c37c-4aac-bf25-0f1cac1b0f26')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-189b0cb0-c37c-4aac-bf25-0f1cac1b0f26 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-189b0cb0-c37c-4aac-bf25-0f1cac1b0f26');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from numpy.core.multiarray import result_type\n",
        "testdata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/test.csv')\n",
        "result_test = pd.DataFrame(columns=['EAP', 'HPL', 'MWS'])\n",
        "for i in range(len(testdata['id'])):\n",
        "  preProcessedTestSentence = nltk.tokenize.word_tokenize(testdata['text'][i].lower())\n",
        "\n",
        "  # create an empy dataframe to put our output in\n",
        "  testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
        "\n",
        "  # For each author...\n",
        "  for j in wordFreqByAuthor.keys():\n",
        "      # for each word in our test sentence...\n",
        "      for k in preProcessedTestSentence:\n",
        "          # find out how frequently the author used that word\n",
        "          wordFreq = wordFreqByAuthor[j].freq(k)\n",
        "          # and add a very small amount to every prob. so none of them are 0\n",
        "          smoothedWordFreq = wordFreq + 0.000001\n",
        "          # add the author, word and smoothed freq. to our dataframe\n",
        "          output = pd.DataFrame([[j, k, smoothedWordFreq]], columns = ['author','word','probability'])\n",
        "          testProbailities = testProbailities.append(output, ignore_index = True)\n",
        "\n",
        "  # empty dataframe for the probability that each author wrote the sentence\n",
        "  testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
        "\n",
        "  # now let's group the dataframe with our frequency by author\n",
        "  for j in wordFreqByAuthor.keys():\n",
        "      # get the joint probability that each author wrote each word\n",
        "      oneAuthor = testProbailities.query('author == \"' + j + '\"')\n",
        "      jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
        "      \n",
        "      # and add that to our dataframe\n",
        "      output = pd.DataFrame([[j, jointProbability]], columns = ['author','jointProbability'])\n",
        "      testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
        "\n",
        "  result_test = result_test.append({\n",
        "      'EAP' : testProbailitiesByAuthor['jointProbability'][0] / testProbailitiesByAuthor['jointProbability'].sum(),\n",
        "      'HPL' : testProbailitiesByAuthor['jointProbability'][1] / testProbailitiesByAuthor['jointProbability'].sum(),\n",
        "      'MWS' : testProbailitiesByAuthor['jointProbability'][2] / testProbailitiesByAuthor['jointProbability'].sum()\n",
        "  },ignore_index=True)\n",
        "\n",
        "result_test.fillna(value=0.33, inplace = True)\n",
        "result_test.to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/wordfreq_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.multiarray import result_type\n",
        "testdata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/train.csv')\n",
        "result_train = pd.DataFrame(columns=['EAP', 'HPL', 'MWS'])\n",
        "for i in range(len(testdata['id'])):\n",
        "  preProcessedTestSentence = nltk.tokenize.word_tokenize(testdata['text'][i].lower())\n",
        "\n",
        "  # create an empy dataframe to put our output in\n",
        "  testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
        "\n",
        "  # For each author...\n",
        "  for j in wordFreqByAuthor.keys():\n",
        "      # for each word in our test sentence...\n",
        "      for k in preProcessedTestSentence:\n",
        "          # find out how frequently the author used that word\n",
        "          wordFreq = wordFreqByAuthor[j].freq(k)\n",
        "          # and add a very small amount to every prob. so none of them are 0\n",
        "          smoothedWordFreq = wordFreq + 0.000001\n",
        "          # add the author, word and smoothed freq. to our dataframe\n",
        "          output = pd.DataFrame([[j, k, smoothedWordFreq]], columns = ['author','word','probability'])\n",
        "          testProbailities = testProbailities.append(output, ignore_index = True)\n",
        "\n",
        "  # empty dataframe for the probability that each author wrote the sentence\n",
        "  testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
        "\n",
        "  # now let's group the dataframe with our frequency by author\n",
        "  for j in wordFreqByAuthor.keys():\n",
        "      # get the joint probability that each author wrote each word\n",
        "      oneAuthor = testProbailities.query('author == \"' + j + '\"')\n",
        "      jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
        "      \n",
        "      # and add that to our dataframe\n",
        "      output = pd.DataFrame([[j, jointProbability]], columns = ['author','jointProbability'])\n",
        "      testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
        "\n",
        "  result_train = result_train.append({\n",
        "      'EAP' : testProbailitiesByAuthor['jointProbability'][0] / testProbailitiesByAuthor['jointProbability'].sum(),\n",
        "      'HPL' : testProbailitiesByAuthor['jointProbability'][1] / testProbailitiesByAuthor['jointProbability'].sum(),\n",
        "      'MWS' : testProbailitiesByAuthor['jointProbability'][2] / testProbailitiesByAuthor['jointProbability'].sum()\n",
        "  },ignore_index=True)\n",
        "\n",
        "result_train.fillna(value=0.33, inplace = True)\n",
        "result_train.to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/wordfreq_train.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G0yrQCAKvj-J",
        "outputId": "8cf86e8a-1e3f-4ce3-ca4f-65320a322705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 / 19579\n",
            "400 / 19579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600 / 19579\n",
            "800 / 19579\n",
            "1000 / 19579\n",
            "1200 / 19579\n",
            "1400 / 19579\n",
            "1600 / 19579\n",
            "1800 / 19579\n",
            "2000 / 19579\n",
            "2200 / 19579\n",
            "2400 / 19579\n",
            "2600 / 19579\n",
            "2800 / 19579\n",
            "3000 / 19579\n",
            "3200 / 19579\n",
            "3400 / 19579\n",
            "3600 / 19579\n",
            "3800 / 19579\n",
            "4000 / 19579\n",
            "4200 / 19579\n",
            "4400 / 19579\n",
            "4600 / 19579\n",
            "4800 / 19579\n",
            "5000 / 19579\n",
            "5200 / 19579\n",
            "5400 / 19579\n",
            "5600 / 19579\n",
            "5800 / 19579\n",
            "6000 / 19579\n",
            "6200 / 19579\n",
            "6400 / 19579\n",
            "6600 / 19579\n",
            "6800 / 19579\n",
            "7000 / 19579\n",
            "7200 / 19579\n",
            "7400 / 19579\n",
            "7600 / 19579\n",
            "7800 / 19579\n",
            "8000 / 19579\n",
            "8200 / 19579\n",
            "8400 / 19579\n",
            "8600 / 19579\n",
            "8800 / 19579\n",
            "9000 / 19579\n",
            "9200 / 19579\n",
            "9400 / 19579\n",
            "9600 / 19579\n",
            "9800 / 19579\n",
            "10000 / 19579\n",
            "10200 / 19579\n",
            "10400 / 19579\n",
            "10600 / 19579\n",
            "10800 / 19579\n",
            "11000 / 19579\n",
            "11200 / 19579\n",
            "11400 / 19579\n",
            "11600 / 19579\n",
            "11800 / 19579\n",
            "12000 / 19579\n",
            "12200 / 19579\n",
            "12400 / 19579\n",
            "12600 / 19579\n",
            "12800 / 19579\n",
            "13000 / 19579\n",
            "13200 / 19579\n",
            "13400 / 19579\n",
            "13600 / 19579\n",
            "13800 / 19579\n",
            "14000 / 19579\n",
            "14200 / 19579\n",
            "14400 / 19579\n",
            "14600 / 19579\n",
            "14800 / 19579\n",
            "15000 / 19579\n",
            "15200 / 19579\n",
            "15400 / 19579\n",
            "15600 / 19579\n",
            "15800 / 19579\n",
            "16000 / 19579\n",
            "16200 / 19579\n",
            "16400 / 19579\n",
            "16600 / 19579\n",
            "16800 / 19579\n",
            "17000 / 19579\n",
            "17200 / 19579\n",
            "17400 / 19579\n",
            "17600 / 19579\n",
            "17800 / 19579\n",
            "18000 / 19579\n",
            "18200 / 19579\n",
            "18400 / 19579\n",
            "18600 / 19579\n",
            "18800 / 19579\n",
            "19000 / 19579\n",
            "19200 / 19579\n",
            "19400 / 19579\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            EAP           HPL           MWS\n",
              "0  1.000000e+00  6.655624e-09  3.207941e-08\n",
              "1  2.679125e-01  7.146617e-01  1.742573e-02\n",
              "2  9.999799e-01  2.007436e-05  9.123353e-11\n",
              "3  2.339326e-13  1.392046e-14  1.000000e+00\n",
              "4  2.917749e-02  9.707895e-01  3.300631e-05"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e27fb23-bd5c-4eab-8d4d-ff23ad544559\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EAP</th>\n",
              "      <th>HPL</th>\n",
              "      <th>MWS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>6.655624e-09</td>\n",
              "      <td>3.207941e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.679125e-01</td>\n",
              "      <td>7.146617e-01</td>\n",
              "      <td>1.742573e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.999799e-01</td>\n",
              "      <td>2.007436e-05</td>\n",
              "      <td>9.123353e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.339326e-13</td>\n",
              "      <td>1.392046e-14</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.917749e-02</td>\n",
              "      <td>9.707895e-01</td>\n",
              "      <td>3.300631e-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e27fb23-bd5c-4eab-8d4d-ff23ad544559')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e27fb23-bd5c-4eab-8d4d-ff23ad544559 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e27fb23-bd5c-4eab-8d4d-ff23ad544559');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOrEi1LVdSxs"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk.stem as stm\n",
        "from nltk import WordNetLemmatizer, word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from time import time\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omsHMPEKd_rq",
        "outputId": "792d1358-0a27-45df-9968-0772bb88a82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "2196018it [03:09, 11564.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2196017 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "start_time = time()\n",
        "color = sns.color_palette()\n",
        "tqdm.pandas()\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "_punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/glove.840B.300d.txt', 'r', errors ='ignore',encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "  values = line.split(' ')\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "    \n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "eng_stopwords = set(stopwords.words(\"english\"))         # for wipe out some common words that hava no help such as \"I\", \"you\", \"and\"\n",
        "pd.options.mode.chained_assignment = None               # prevent raise an exception\n",
        "alpha_tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxVTIV70eAbO",
        "outputId": "cb1ae1fa-ccc5-44d6-8160-93f8dacff568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in train dataset :  19579\n",
            "Number of rows in test dataset :  8392\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/test.csv\")\n",
        "print(\"Number of rows in train dataset : \",train_df.shape[0])\n",
        "print(\"Number of rows in test dataset : \",test_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordfreq_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/wordfreq_train.csv\")\n",
        "wordfreq_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/wordfreq_test.csv\")\n",
        "train_df = pd.concat([train_df, wordfreq_train], axis=1)\n",
        "test_df = pd.concat([test_df, wordfreq_test], axis=1)"
      ],
      "metadata": {
        "id": "lgK1PQgDxNTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg5fk1EMaN-e"
      },
      "outputs": [],
      "source": [
        "def fraction_noun(row):\n",
        "    \"\"\"function to give us fraction of noun over total words \"\"\"\n",
        "    text = row['text']\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    pos_list = nltk.pos_tag(text_splited)\n",
        "    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
        "    return (noun_count/word_count)\n",
        "\n",
        "def fraction_adj(row):\n",
        "    \"\"\"function to give us fraction of adjectives over total words in given text\"\"\"\n",
        "    text = row['text']\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    pos_list = nltk.pos_tag(text_splited)\n",
        "    adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
        "    return (adj_count/word_count)\n",
        "\n",
        "def fraction_verbs(row):\n",
        "    \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n",
        "    text = row['text']\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    pos_list = nltk.pos_tag(text_splited)\n",
        "    verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
        "    return (verbs_count/word_count)\n",
        "\n",
        "def fraction_adverbs(row):\n",
        "    \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n",
        "    text = row['text']\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    pos_list = nltk.pos_tag(text_splited)\n",
        "    verbs_count = len([w for w in pos_list if w[1] in ('RB','RBR','RBS')])\n",
        "    return (verbs_count/word_count)\n",
        "\n",
        "\n",
        "def clean_text(x):\n",
        "    x.lower()\n",
        "    for p in _punctuation:\n",
        "        x.replace(p, '')\n",
        "    return x\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower() #.decode('utf-8')\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in eng_stopwords]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(embeddings_index[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    if type(v) != np.ndarray:\n",
        "        return np.zeros(300)\n",
        "    return v / np.sqrt((v ** 2).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "XhEx90wUeUlD",
        "outputId": "17d6c697-72b4-4f3f-8efc-edeab6464312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                               text author  \\\n",
              "0  id26305  This process, however, afforded me no means of...    EAP   \n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
              "\n",
              "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
              "0         41                35        231             19                 7   \n",
              "1         14                14         71              8                 1   \n",
              "2         36                32        200             16                 5   \n",
              "3         34                32        206             13                 4   \n",
              "4         27                25        174             11                 4   \n",
              "\n",
              "   num_words_upper  num_words_title  ...  num_q  num_r  num_s  num_t  num_u  \\\n",
              "0                2                3  ...      0     10     12     17      6   \n",
              "1                0                1  ...      0      4      1      7      2   \n",
              "2                0                1  ...      0      5     16     15      3   \n",
              "3                0                4  ...      0     14     12     10      1   \n",
              "4                0                2  ...      0      3     12     14      3   \n",
              "\n",
              "   num_v  num_w  num_x  num_y  num_z  \n",
              "0      1      5      0      2      0  \n",
              "1      1      0      0      0      0  \n",
              "2      0      4      1      1      0  \n",
              "3      1      4      1      4      0  \n",
              "4      2      0      1      1      0  \n",
              "\n",
              "[5 rows x 158 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b0035a4-341d-48f4-a1ff-36c3bfdb647a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_stopwords</th>\n",
              "      <th>num_punctuations</th>\n",
              "      <th>num_words_upper</th>\n",
              "      <th>num_words_title</th>\n",
              "      <th>...</th>\n",
              "      <th>num_q</th>\n",
              "      <th>num_r</th>\n",
              "      <th>num_s</th>\n",
              "      <th>num_t</th>\n",
              "      <th>num_u</th>\n",
              "      <th>num_v</th>\n",
              "      <th>num_w</th>\n",
              "      <th>num_x</th>\n",
              "      <th>num_y</th>\n",
              "      <th>num_z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "      <td>41</td>\n",
              "      <td>35</td>\n",
              "      <td>231</td>\n",
              "      <td>19</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>71</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "      <td>36</td>\n",
              "      <td>32</td>\n",
              "      <td>200</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "      <td>34</td>\n",
              "      <td>32</td>\n",
              "      <td>206</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>174</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  158 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b0035a4-341d-48f4-a1ff-36c3bfdb647a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b0035a4-341d-48f4-a1ff-36c3bfdb647a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b0035a4-341d-48f4-a1ff-36c3bfdb647a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "## Number of unique words in the text ##\n",
        "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "## Number of characters in the text ##\n",
        "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
        "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "## Number of stopwords in the text ##\n",
        "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "\n",
        "## Number of punctuations in the text ##\n",
        "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "## Average length of the words in the text ##\n",
        "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "train_df[\",\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\",\")]))\n",
        "test_df[\",\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\",\")]))\n",
        "\n",
        "train_df[\";\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\";\")]))\n",
        "test_df[\";\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\";\")]))\n",
        "\n",
        "train_df['\\\"'] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split('\\\"')]))\n",
        "test_df['\\\"'] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split('\\\"')]))\n",
        "\n",
        "train_df[\"...\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"...\")]))\n",
        "test_df[\"...\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"...\")]))\n",
        "\n",
        "train_df[\"?\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"?\")]))\n",
        "test_df[\"?\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"?\")]))\n",
        "\n",
        "train_df[\"!\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"!\")]))\n",
        "test_df[\"!\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"!\")]))\n",
        "\n",
        "train_df[\".\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\".\")]))\n",
        "test_df[\".\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\".\")]))\n",
        "\n",
        "train_df[\":\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\":\")]))\n",
        "test_df[\":\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\":\")]))\n",
        "\n",
        "train_df[\"*\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"*\")]))\n",
        "test_df[\"*\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"*\")]))\n",
        "\n",
        "train_df[\"-\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"-\")]))\n",
        "test_df[\"-\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split(\"-\")]))\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "train_df['fraction_noun'] = train_df.apply(lambda row: fraction_noun(row), axis =1)\n",
        "test_df['fraction_noun'] = test_df.apply(lambda row: fraction_noun(row), axis =1)\n",
        "\n",
        "train_df['fraction_adj'] = train_df.apply(lambda row: fraction_adj(row), axis =1)\n",
        "test_df['fraction_adj'] = test_df.apply(lambda row: fraction_adj(row), axis =1)\n",
        "\n",
        "train_df['fraction_verbs'] = train_df.apply(lambda row: fraction_verbs(row), axis =1)\n",
        "test_df['fraction_verbs'] = test_df.apply(lambda row: fraction_verbs(row), axis =1)\n",
        "\n",
        "\n",
        "most_words = ['strange', 'night', 'ancient', 'terrible', 'house', 'street', 'black', 'dark', 'city', 'remain', ''\n",
        "          'moon', 'west', 'told', 'looked', 'dreams', 'door', 'stone', 'half', 'left','found', 'course', 'observe',\n",
        "          'head', 'person', 'length', 'water', 'character', 'moment', 'manner', 'air', 'ider', 'speak', 'place',\n",
        "            'hand', 'matter', 'de', 'feet', 'body', 'means', 'doubt','raymond', 'perdita', 'adrian', 'fall', 'come',\n",
        "            'father', 'country', 'heart', 'idris', 'spirit', 'love', 'life', 'say', 'find', 'thing', 'long', 'dream',\n",
        "          'idris', 'tears', 'passed', 'nature', 'fear', 'human', 'voice', 'dear', 'words', 'great', 'little', 'see',\n",
        "          'the ', ' a ', 'appear', 'little', 'was ', 'one ', 'two ', 'three ', 'ten ', 'is ', 'are ', 'ed ', 'misery',\n",
        "            'however', ' to ', 'into', 'about ', 'th', 'er', 'ex', 'an ', 'ground', 'any', 'silence', 'wall', 'look'\n",
        "            , 'The ', 'I ', 'It ', 'He', 'Me', 'They ', 'She ', 'We ', 'You ', 'good', 'time', 'old', 'death', 'man']\n",
        "\n",
        "_punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
        "\n",
        "train_df['text_cleaned'] = train_df['text'].apply(lambda x: clean_text(x))\n",
        "test_df['text_cleaned'] = test_df['text'].apply(lambda x: clean_text(x))\n",
        "\n",
        "for word in most_words:\n",
        "    train_df[word] = train_df[\"text_cleaned\"].str.count(word)\n",
        "    test_df[word] = test_df[\"text_cleaned\"].str.count(word)\n",
        "\n",
        "for char in alphabet:\n",
        "    train_df['num_'+char] = train_df[\"text_cleaned\"].str.count(char)\n",
        "    test_df['num_'+char] = test_df[\"text_cleaned\"].str.count(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUb1uYyDrc_W",
        "outputId": "0dca6603-b0f2-4ceb-e1dc-5c4062045843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "100%|| 19579/19579 [00:07<00:00, 2736.28it/s]\n",
            "100%|| 8392/8392 [00:03<00:00, 2786.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19579, 300) (8392, 300)\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "xtrain_glove = [sent2vec(x) for x in tqdm(train_df.text)]\n",
        "xtest_glove = [sent2vec(x) for x in tqdm(test_df.text)]\n",
        "xtrain_glove = np.array(xtrain_glove)\n",
        "xtest_glove = np.array(xtest_glove)\n",
        "print(xtrain_glove.shape, xtest_glove.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjLAat24sDd-",
        "outputId": "0562cd40-29f2-43de-cee8-9f5e2f2ceca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19579 entries, 0 to 19578\n",
            "Columns: 461 entries, id to 299\n",
            "dtypes: float64(317), int64(140), object(4)\n",
            "memory usage: 68.9+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.concat([train_df, pd.DataFrame(xtrain_glove)], axis=1)\n",
        "test_df = pd.concat([test_df, pd.DataFrame(xtest_glove)], axis=1)\n",
        "print(train_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EznV9aqog9Bk"
      },
      "outputs": [],
      "source": [
        "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
        "train_y = train_df['author'].map(author_mapping_dict)\n",
        "train_id = train_df['id'].values\n",
        "test_id = test_df['id'].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [('MultiNB', MultinomialNB(alpha=0.03)),\n",
        "          ('Calibrated MultiNB', CalibratedClassifierCV(\n",
        "              MultinomialNB(alpha=0.03), method='isotonic')),\n",
        "          ('Calibrated BernoulliNB', CalibratedClassifierCV(\n",
        "              BernoulliNB(alpha=0.03), method='isotonic')),\n",
        "          ('Calibrated Huber', CalibratedClassifierCV(\n",
        "              SGDClassifier(loss='modified_huber', alpha=1e-4,\n",
        "                            max_iter=10000, tol=1e-4), method='sigmoid')),\n",
        "          ('Logit', LogisticRegression(C=30, max_iter=10000))]"
      ],
      "metadata": {
        "id": "gj2UFj-ukMSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[2,3,3,1,1])\n",
        "\n",
        "tfidf_vec = CountVectorizer(analyzer='word', ngram_range=(1, 5))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_2.csv\", index=False)\n",
        "\n",
        "\n",
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[2,3,3,1,1])\n",
        "\n",
        "tfidf_vec = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_c2_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_c2_2.csv\", index=False)"
      ],
      "metadata": {
        "id": "Gb9hzYOEkVPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_word2_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_1.csv\")\n",
        "nb_word2_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_2.csv\")\n",
        "train_df[\"nb_word2_eap\"] = nb_word2_1.iloc[:,[0]]\n",
        "train_df[\"nb_word2_hpl\"] = nb_word2_1.iloc[:,[1]]\n",
        "train_df[\"nb_word2_mws\"] = nb_word2_1.iloc[:,[2]]\n",
        "test_df[\"nb_word2_eap\"] = nb_word2_2.iloc[:,[0]]\n",
        "test_df[\"nb_word2_hpl\"] = nb_word2_2.iloc[:,[1]]\n",
        "test_df[\"nb_word2_mws\"] = nb_word2_2.iloc[:,[2]]\n",
        "nb_c2_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_c2_1.csv\")\n",
        "nb_c2_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_c2_2.csv\")\n",
        "train_df[\"nb_c2_eap\"] = nb_c2_1.iloc[:,[0]]\n",
        "train_df[\"nb_c2_hpl\"] = nb_c2_1.iloc[:,[1]]\n",
        "train_df[\"nb_c2_mws\"] = nb_c2_1.iloc[:,[2]]\n",
        "test_df[\"nb_c2_eap\"] = nb_c2_2.iloc[:,[0]]\n",
        "test_df[\"nb_c2_hpl\"] = nb_c2_2.iloc[:,[1]]\n",
        "test_df[\"nb_c2_mws\"] = nb_c2_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "h2jR4gdkkh9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[2,3,3,1,1])\n",
        "\n",
        "tfidf_vec = CountVectorizer(analyzer='word', ngram_range=(1, 5))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runBer(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_word2_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_word2_2.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[3,2,3,1,1])\n",
        "\n",
        "tfidf_vec = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runBer(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_c2_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_c2_2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "AxLOGxX-kl78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ber_word2_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_1.csv\")\n",
        "ber_word2_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_2.csv\")\n",
        "train_df[\"ber_word2_eap\"] = ber_word2_1.iloc[:,[0]]\n",
        "train_df[\"ber_word2_hpl\"] = ber_word2_1.iloc[:,[1]]\n",
        "train_df[\"ber_word2_mws\"] = ber_word2_1.iloc[:,[2]]\n",
        "test_df[\"ber_word2_eap\"] = ber_word2_2.iloc[:,[0]]\n",
        "test_df[\"ber_word2_hpl\"] = ber_word2_2.iloc[:,[1]]\n",
        "test_df[\"ber_word2_mws\"] = ber_word2_2.iloc[:,[2]]\n",
        "ber_c2_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_1.csv\")\n",
        "ber_c2_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_word2_2.csv\")\n",
        "train_df[\"ber_c2_eap\"] = ber_c2_1.iloc[:,[0]]\n",
        "train_df[\"ber_c2_hpl\"] = ber_c2_1.iloc[:,[1]]\n",
        "train_df[\"ber_c2_mws\"] = ber_c2_1.iloc[:,[2]]\n",
        "test_df[\"ber_c2_eap\"] = ber_c2_2.iloc[:,[0]]\n",
        "test_df[\"ber_c2_hpl\"] = ber_c2_2.iloc[:,[1]]\n",
        "test_df[\"ber_c2_mws\"] = ber_c2_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "_RkJ9t8UkrNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(full_tfidf)\n",
        "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
        "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
        "\n",
        "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "train_df = pd.concat([train_df, train_svd], axis=1)\n",
        "test_df = pd.concat([test_df, test_svd], axis=1)\n",
        "\n",
        "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
      ],
      "metadata": {
        "id": "flZlwe9akxWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[2,3,3,1,1])\n",
        "\n",
        "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_cvec_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_cvec_2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "WHj-01I0kzsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_cvec_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_cvec_1.csv\")\n",
        "nb_cvec_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_cvec_2.csv\")\n",
        "train_df[\"nb_cvec_eap\"] = nb_cvec_1.iloc[:,[0]]\n",
        "train_df[\"nb_cvec_hpl\"] = nb_cvec_1.iloc[:,[1]]\n",
        "train_df[\"nb_cvec_mws\"] = nb_cvec_1.iloc[:,[2]]\n",
        "test_df[\"nb_cvec_eap\"] = nb_cvec_2.iloc[:,[0]]\n",
        "test_df[\"nb_cvec_hpl\"] = nb_cvec_2.iloc[:,[1]]\n",
        "test_df[\"nb_cvec_mws\"] = nb_cvec_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "34gIl6cQk2HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[3,2,3,1,1])\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runBer(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_cvec_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_cvec_2.csv\", index=False)"
      ],
      "metadata": {
        "id": "Yct_XY38k4jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ber_cvec_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_cvec_1.csv\")\n",
        "ber_cvec_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_cvec_2.csv\")\n",
        "train_df[\"ber_cvec_eap\"] = ber_cvec_1.iloc[:,[0]]\n",
        "train_df[\"ber_cvec_hpl\"] = ber_cvec_1.iloc[:,[1]]\n",
        "train_df[\"ber_cvec_mws\"] = ber_cvec_1.iloc[:,[2]]\n",
        "test_df[\"ber_cvec_eap\"] = ber_cvec_2.iloc[:,[0]]\n",
        "test_df[\"ber_cvec_hpl\"] = ber_cvec_2.iloc[:,[1]]\n",
        "test_df[\"ber_cvec_mws\"] = ber_cvec_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "F2sr16H2k7iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[3,2,3,1,1])\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
        "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf_char_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf_char_2.csv\", index=False)\n",
        "\n",
        "\n",
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[2,3,3,1,1])\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runBer(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf_char_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf_char_2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ksofoRzMk-oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_tfidf_char_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf_char_1.csv\")\n",
        "nb_tfidf_char_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf_char_2.csv\")\n",
        "train_df[\"nb_tfidf_char_eap\"] = nb_tfidf_char_1.iloc[:,[0]]\n",
        "train_df[\"nb_tfidf_char_hpl\"] = nb_tfidf_char_1.iloc[:,[1]]\n",
        "train_df[\"nb_tfidf_char_mws\"] = nb_tfidf_char_1.iloc[:,[2]]\n",
        "test_df[\"nb_tfidf_char_eap\"] = nb_tfidf_char_2.iloc[:,[0]]\n",
        "test_df[\"nb_tfidf_char_hpl\"] = nb_tfidf_char_2.iloc[:,[1]]\n",
        "test_df[\"nb_tfidf_char_mws\"] = nb_tfidf_char_2.iloc[:,[2]]\n",
        "ber_tfidf_char_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf_char_1.csv\")\n",
        "ber_tfidf_char_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf_char_2.csv\")\n",
        "train_df[\"ber_tfidf_char_eap\"] = ber_tfidf_char_1.iloc[:,[0]]\n",
        "train_df[\"ber_tfidf_char_hpl\"] = ber_tfidf_char_1.iloc[:,[1]]\n",
        "train_df[\"ber_tfidf_char_mws\"] = ber_tfidf_char_1.iloc[:,[2]]\n",
        "test_df[\"ber_tfidf_char_eap\"] = ber_tfidf_char_2.iloc[:,[0]]\n",
        "test_df[\"ber_tfidf_char_hpl\"] = ber_tfidf_char_2.iloc[:,[1]]\n",
        "test_df[\"ber_tfidf_char_mws\"] = ber_tfidf_char_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "ZZBhSOMzlCQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[2,3,3,1,1])\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='word')\n",
        "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runBer(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf2_char_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf2_char_2.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[3,2,3,1,1])\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_test_y = clf.predict_proba(test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf2_char_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf2_char_2.csv\", index=False)"
      ],
      "metadata": {
        "id": "l9B7SrIalEnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ber_tfidf2_char_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf2_char_1.csv\")\n",
        "ber_tfidf2_char_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/ber_tfidf2_char_2.csv\")\n",
        "train_df[\"ber_tfidf2_char_eap\"] = ber_tfidf2_char_1.iloc[:,[0]]\n",
        "train_df[\"ber_tfidf2_char_hpl\"] = ber_tfidf2_char_1.iloc[:,[1]]\n",
        "train_df[\"ber_tfidf2_char_mws\"] = ber_tfidf2_char_1.iloc[:,[2]]\n",
        "test_df[\"ber_tfidf2_char_eap\"] = ber_tfidf2_char_2.iloc[:,[0]]\n",
        "test_df[\"ber_tfidf2_char_hpl\"] = ber_tfidf2_char_2.iloc[:,[1]]\n",
        "test_df[\"ber_tfidf2_char_mws\"] = ber_tfidf2_char_2.iloc[:,[2]]\n",
        "nb_tfidf2_char_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf2_char_1.csv\")\n",
        "nb_tfidf2_char_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/nb_tfidf2_char_2.csv\")\n",
        "train_df[\"nb_tfidf2_char_eap\"] = nb_tfidf2_char_1.iloc[:,[0]]\n",
        "train_df[\"nb_tfidf2_char_hpl\"] = nb_tfidf2_char_1.iloc[:,[1]]\n",
        "train_df[\"nb_tfidf2_char_mws\"] = nb_tfidf2_char_1.iloc[:,[2]]\n",
        "test_df[\"nb_tfidf2_char_eap\"] = nb_tfidf2_char_2.iloc[:,[0]]\n",
        "test_df[\"nb_tfidf2_char_hpl\"] = nb_tfidf2_char_2.iloc[:,[1]]\n",
        "test_df[\"nb_tfidf2_char_mws\"] = nb_tfidf2_char_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "DD5EyPq_lIdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='word')\n",
        "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(full_tfidf)\n",
        "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
        "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
        "\n",
        "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "train_df = pd.concat([train_df, train_svd], axis=1)\n",
        "test_df = pd.concat([test_df, test_svd], axis=1)\n",
        "\n",
        "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
      ],
      "metadata": {
        "id": "Lt8hgvVNlKie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
        "clf = VotingClassifier(models, voting='soft', weights=[3,3,3,1,1])\n",
        "X_train = vectorizer.fit_transform(train_df.text.values)\n",
        "authors = ['MWS','EAP','HPL']\n",
        "y_train = train_df.author.apply(authors.index).values\n",
        "X_test = vectorizer.transform(test_df.text.values)\n",
        "\n",
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "for dev_index, val_index in kf.split(X_train):\n",
        "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
        "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
        "    clf.fit(dev_X, dev_y)\n",
        "    pred_val_y = clf.predict_proba(val_X)\n",
        "    pred_full_test = pred_full_test + clf.predict_proba(X_test)\n",
        "    #pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "    break\n",
        "print(\"cv scores : \", cv_scores)\n",
        "pred_full_test /= 10\n",
        "\n",
        "pd.DataFrame(pred_train).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_train).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/em_tfidf_1.csv\", index=False)\n",
        "pd.DataFrame(pred_full_test).fillna(value=0.33, inplace = True)\n",
        "pd.DataFrame(pred_full_test).to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/em_tfidf_2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "IMex6lpxlM4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "em_tfidf_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/em_tfidf_1.csv\")\n",
        "em_tfidf_2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/em_tfidf_2.csv\")\n",
        "train_df[\"em_tfidf_eap\"] = em_tfidf_1.iloc[:,[0]]\n",
        "train_df[\"em_tfidf_hpl\"] = em_tfidf_1.iloc[:,[1]]\n",
        "train_df[\"em_tfidf_mws\"] = em_tfidf_1.iloc[:,[2]]\n",
        "test_df[\"em_tfidf_eap\"] = em_tfidf_2.iloc[:,[0]]\n",
        "test_df[\"em_tfidf_hpl\"] = em_tfidf_2.iloc[:,[1]]\n",
        "test_df[\"em_tfidf_mws\"] = em_tfidf_2.iloc[:,[2]]"
      ],
      "metadata": {
        "id": "W-ES1dm9lPcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = ['id', 'text', 'text_cleaned']\n",
        "train_X = train_df.drop(cols_to_drop+['author'], axis=1)\n",
        "test_X = test_df.drop(cols_to_drop, axis=1)"
      ],
      "metadata": {
        "id": "lJD7m9v5kKts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J5hNju5hXRU"
      },
      "outputs": [],
      "source": [
        "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.1\n",
        "    param['max_depth'] = 3\n",
        "    param['silent'] = 1\n",
        "    param['num_class'] = 3\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = child\n",
        "    param['subsample'] = 0.8\n",
        "    param['colsample_bytree'] = colsample\n",
        "    param['seed'] = seed_val\n",
        "    num_rounds = 2000\n",
        "\n",
        "    plst = list(param.items())\n",
        "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
        "\n",
        "    if test_y is not None:\n",
        "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
        "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
        "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
        "    else:\n",
        "        xgtest = xgb.DMatrix(test_X)\n",
        "        model = xgb.train(plst, xgtrain, num_rounds)\n",
        "\n",
        "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
        "    if test_X2 is not None:\n",
        "        xgtest2 = xgb.DMatrix(test_X2)\n",
        "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
        "    return pred_test_y, pred_test_y2, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2_z2fHjh-wZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cad0cb8-e30d-4bae-ac98-b72789449d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:1.01072\ttest-mlogloss:1.01031\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.367898\ttest-mlogloss:0.371526\n",
            "[40]\ttrain-mlogloss:0.211503\ttest-mlogloss:0.218653\n",
            "[60]\ttrain-mlogloss:0.174903\ttest-mlogloss:0.187617\n",
            "[80]\ttrain-mlogloss:0.155049\ttest-mlogloss:0.175103\n",
            "[100]\ttrain-mlogloss:0.141749\ttest-mlogloss:0.1697\n",
            "[120]\ttrain-mlogloss:0.130432\ttest-mlogloss:0.165871\n",
            "[140]\ttrain-mlogloss:0.121234\ttest-mlogloss:0.163178\n",
            "[160]\ttrain-mlogloss:0.112782\ttest-mlogloss:0.160636\n",
            "[180]\ttrain-mlogloss:0.105563\ttest-mlogloss:0.158883\n",
            "[200]\ttrain-mlogloss:0.099002\ttest-mlogloss:0.15754\n",
            "[220]\ttrain-mlogloss:0.092465\ttest-mlogloss:0.156872\n",
            "[240]\ttrain-mlogloss:0.086862\ttest-mlogloss:0.156478\n",
            "[260]\ttrain-mlogloss:0.081259\ttest-mlogloss:0.155627\n",
            "[280]\ttrain-mlogloss:0.076335\ttest-mlogloss:0.155339\n",
            "[300]\ttrain-mlogloss:0.072071\ttest-mlogloss:0.155982\n",
            "[320]\ttrain-mlogloss:0.067904\ttest-mlogloss:0.155257\n",
            "[340]\ttrain-mlogloss:0.063841\ttest-mlogloss:0.155312\n",
            "[360]\ttrain-mlogloss:0.060025\ttest-mlogloss:0.154553\n",
            "[380]\ttrain-mlogloss:0.056318\ttest-mlogloss:0.15506\n",
            "[400]\ttrain-mlogloss:0.052861\ttest-mlogloss:0.15507\n",
            "Stopping. Best iteration:\n",
            "[360]\ttrain-mlogloss:0.060025\ttest-mlogloss:0.154553\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01046\ttest-mlogloss:1.01237\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.367883\ttest-mlogloss:0.372966\n",
            "[40]\ttrain-mlogloss:0.211795\ttest-mlogloss:0.21679\n",
            "[60]\ttrain-mlogloss:0.174996\ttest-mlogloss:0.184042\n",
            "[80]\ttrain-mlogloss:0.155642\ttest-mlogloss:0.171212\n",
            "[100]\ttrain-mlogloss:0.142646\ttest-mlogloss:0.16647\n",
            "[120]\ttrain-mlogloss:0.131451\ttest-mlogloss:0.163676\n",
            "[140]\ttrain-mlogloss:0.12177\ttest-mlogloss:0.161556\n",
            "[160]\ttrain-mlogloss:0.11338\ttest-mlogloss:0.15984\n",
            "[180]\ttrain-mlogloss:0.105785\ttest-mlogloss:0.158523\n",
            "[200]\ttrain-mlogloss:0.099333\ttest-mlogloss:0.1584\n",
            "[220]\ttrain-mlogloss:0.092979\ttest-mlogloss:0.157183\n",
            "[240]\ttrain-mlogloss:0.086936\ttest-mlogloss:0.157382\n",
            "[260]\ttrain-mlogloss:0.081472\ttest-mlogloss:0.156942\n",
            "[280]\ttrain-mlogloss:0.076347\ttest-mlogloss:0.156056\n",
            "[300]\ttrain-mlogloss:0.071866\ttest-mlogloss:0.156086\n",
            "[320]\ttrain-mlogloss:0.067493\ttest-mlogloss:0.156568\n",
            "[340]\ttrain-mlogloss:0.063443\ttest-mlogloss:0.156572\n",
            "Stopping. Best iteration:\n",
            "[307]\ttrain-mlogloss:0.070303\ttest-mlogloss:0.155813\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01068\ttest-mlogloss:1.01092\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.368567\ttest-mlogloss:0.371132\n",
            "[40]\ttrain-mlogloss:0.212066\ttest-mlogloss:0.216558\n",
            "[60]\ttrain-mlogloss:0.175215\ttest-mlogloss:0.18454\n",
            "[80]\ttrain-mlogloss:0.155504\ttest-mlogloss:0.17057\n",
            "[100]\ttrain-mlogloss:0.142562\ttest-mlogloss:0.164409\n",
            "[120]\ttrain-mlogloss:0.131884\ttest-mlogloss:0.161331\n",
            "[140]\ttrain-mlogloss:0.122276\ttest-mlogloss:0.158469\n",
            "[160]\ttrain-mlogloss:0.113878\ttest-mlogloss:0.156745\n",
            "[180]\ttrain-mlogloss:0.106292\ttest-mlogloss:0.155294\n",
            "[200]\ttrain-mlogloss:0.09982\ttest-mlogloss:0.154683\n",
            "[220]\ttrain-mlogloss:0.093509\ttest-mlogloss:0.153742\n",
            "[240]\ttrain-mlogloss:0.087773\ttest-mlogloss:0.152502\n",
            "[260]\ttrain-mlogloss:0.082455\ttest-mlogloss:0.152283\n",
            "[280]\ttrain-mlogloss:0.077863\ttest-mlogloss:0.151388\n",
            "[300]\ttrain-mlogloss:0.073063\ttest-mlogloss:0.150399\n",
            "[320]\ttrain-mlogloss:0.068768\ttest-mlogloss:0.150399\n",
            "[340]\ttrain-mlogloss:0.064559\ttest-mlogloss:0.15003\n",
            "[360]\ttrain-mlogloss:0.060786\ttest-mlogloss:0.149845\n",
            "[380]\ttrain-mlogloss:0.056906\ttest-mlogloss:0.149457\n",
            "[400]\ttrain-mlogloss:0.053532\ttest-mlogloss:0.149125\n",
            "[420]\ttrain-mlogloss:0.050374\ttest-mlogloss:0.149271\n",
            "[440]\ttrain-mlogloss:0.047473\ttest-mlogloss:0.14961\n",
            "Stopping. Best iteration:\n",
            "[392]\ttrain-mlogloss:0.054867\ttest-mlogloss:0.14844\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01049\ttest-mlogloss:1.01245\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.36627\ttest-mlogloss:0.386023\n",
            "[40]\ttrain-mlogloss:0.20951\ttest-mlogloss:0.237264\n",
            "[60]\ttrain-mlogloss:0.172583\ttest-mlogloss:0.206996\n",
            "[80]\ttrain-mlogloss:0.15309\ttest-mlogloss:0.194616\n",
            "[100]\ttrain-mlogloss:0.139665\ttest-mlogloss:0.189442\n",
            "[120]\ttrain-mlogloss:0.128273\ttest-mlogloss:0.185873\n",
            "[140]\ttrain-mlogloss:0.119282\ttest-mlogloss:0.183984\n",
            "[160]\ttrain-mlogloss:0.111151\ttest-mlogloss:0.182026\n",
            "[180]\ttrain-mlogloss:0.103762\ttest-mlogloss:0.181719\n",
            "[200]\ttrain-mlogloss:0.097142\ttest-mlogloss:0.180854\n",
            "[220]\ttrain-mlogloss:0.090776\ttest-mlogloss:0.180812\n",
            "[240]\ttrain-mlogloss:0.085157\ttest-mlogloss:0.180917\n",
            "[260]\ttrain-mlogloss:0.079931\ttest-mlogloss:0.180604\n",
            "Stopping. Best iteration:\n",
            "[213]\ttrain-mlogloss:0.092959\ttest-mlogloss:0.180523\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01065\ttest-mlogloss:1.01172\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.367407\ttest-mlogloss:0.375471\n",
            "[40]\ttrain-mlogloss:0.211043\ttest-mlogloss:0.223031\n",
            "[60]\ttrain-mlogloss:0.174534\ttest-mlogloss:0.19192\n",
            "[80]\ttrain-mlogloss:0.154931\ttest-mlogloss:0.179227\n",
            "[100]\ttrain-mlogloss:0.14185\ttest-mlogloss:0.1732\n",
            "[120]\ttrain-mlogloss:0.130713\ttest-mlogloss:0.168819\n",
            "[140]\ttrain-mlogloss:0.121319\ttest-mlogloss:0.165552\n",
            "[160]\ttrain-mlogloss:0.113171\ttest-mlogloss:0.163965\n",
            "[180]\ttrain-mlogloss:0.10541\ttest-mlogloss:0.16303\n",
            "[200]\ttrain-mlogloss:0.098476\ttest-mlogloss:0.161166\n",
            "[220]\ttrain-mlogloss:0.092373\ttest-mlogloss:0.161318\n",
            "[240]\ttrain-mlogloss:0.086457\ttest-mlogloss:0.161109\n",
            "[260]\ttrain-mlogloss:0.080936\ttest-mlogloss:0.160317\n",
            "[280]\ttrain-mlogloss:0.075911\ttest-mlogloss:0.160955\n",
            "[300]\ttrain-mlogloss:0.07106\ttest-mlogloss:0.160751\n",
            "Stopping. Best iteration:\n",
            "[260]\ttrain-mlogloss:0.080936\ttest-mlogloss:0.160317\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01058\ttest-mlogloss:1.01093\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.366775\ttest-mlogloss:0.379494\n",
            "[40]\ttrain-mlogloss:0.209646\ttest-mlogloss:0.233007\n",
            "[60]\ttrain-mlogloss:0.172855\ttest-mlogloss:0.204345\n",
            "[80]\ttrain-mlogloss:0.152936\ttest-mlogloss:0.193324\n",
            "[100]\ttrain-mlogloss:0.139588\ttest-mlogloss:0.188631\n",
            "[120]\ttrain-mlogloss:0.128408\ttest-mlogloss:0.185497\n",
            "[140]\ttrain-mlogloss:0.119247\ttest-mlogloss:0.183735\n",
            "[160]\ttrain-mlogloss:0.111116\ttest-mlogloss:0.182779\n",
            "[180]\ttrain-mlogloss:0.103654\ttest-mlogloss:0.181953\n",
            "[200]\ttrain-mlogloss:0.097064\ttest-mlogloss:0.18165\n",
            "[220]\ttrain-mlogloss:0.090924\ttest-mlogloss:0.181111\n",
            "[240]\ttrain-mlogloss:0.085194\ttest-mlogloss:0.180635\n",
            "[260]\ttrain-mlogloss:0.079695\ttest-mlogloss:0.180413\n",
            "[280]\ttrain-mlogloss:0.074934\ttest-mlogloss:0.179993\n",
            "[300]\ttrain-mlogloss:0.07052\ttest-mlogloss:0.179905\n",
            "[320]\ttrain-mlogloss:0.066244\ttest-mlogloss:0.180494\n",
            "[340]\ttrain-mlogloss:0.062295\ttest-mlogloss:0.180977\n",
            "Stopping. Best iteration:\n",
            "[301]\ttrain-mlogloss:0.070323\ttest-mlogloss:0.179722\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01091\ttest-mlogloss:1.01031\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.368962\ttest-mlogloss:0.362747\n",
            "[40]\ttrain-mlogloss:0.212824\ttest-mlogloss:0.208733\n",
            "[60]\ttrain-mlogloss:0.176082\ttest-mlogloss:0.177198\n",
            "[80]\ttrain-mlogloss:0.15662\ttest-mlogloss:0.164861\n",
            "[100]\ttrain-mlogloss:0.143625\ttest-mlogloss:0.159148\n",
            "[120]\ttrain-mlogloss:0.132743\ttest-mlogloss:0.154806\n",
            "[140]\ttrain-mlogloss:0.123498\ttest-mlogloss:0.152258\n",
            "[160]\ttrain-mlogloss:0.114889\ttest-mlogloss:0.15051\n",
            "[180]\ttrain-mlogloss:0.107366\ttest-mlogloss:0.148509\n",
            "[200]\ttrain-mlogloss:0.100596\ttest-mlogloss:0.147191\n",
            "[220]\ttrain-mlogloss:0.094237\ttest-mlogloss:0.146082\n",
            "[240]\ttrain-mlogloss:0.087906\ttest-mlogloss:0.145647\n",
            "[260]\ttrain-mlogloss:0.082535\ttest-mlogloss:0.145083\n",
            "[280]\ttrain-mlogloss:0.077439\ttest-mlogloss:0.145105\n",
            "[300]\ttrain-mlogloss:0.072948\ttest-mlogloss:0.145254\n",
            "[320]\ttrain-mlogloss:0.068441\ttest-mlogloss:0.145052\n",
            "[340]\ttrain-mlogloss:0.064606\ttest-mlogloss:0.145378\n",
            "[360]\ttrain-mlogloss:0.060699\ttest-mlogloss:0.145057\n",
            "Stopping. Best iteration:\n",
            "[318]\ttrain-mlogloss:0.068944\ttest-mlogloss:0.144809\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01066\ttest-mlogloss:1.01066\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.367643\ttest-mlogloss:0.374002\n",
            "[40]\ttrain-mlogloss:0.21096\ttest-mlogloss:0.22464\n",
            "[60]\ttrain-mlogloss:0.174143\ttest-mlogloss:0.194121\n",
            "[80]\ttrain-mlogloss:0.15429\ttest-mlogloss:0.181903\n",
            "[100]\ttrain-mlogloss:0.141184\ttest-mlogloss:0.176901\n",
            "[120]\ttrain-mlogloss:0.129777\ttest-mlogloss:0.173781\n",
            "[140]\ttrain-mlogloss:0.120411\ttest-mlogloss:0.171006\n",
            "[160]\ttrain-mlogloss:0.111959\ttest-mlogloss:0.169128\n",
            "[180]\ttrain-mlogloss:0.104607\ttest-mlogloss:0.168364\n",
            "[200]\ttrain-mlogloss:0.097416\ttest-mlogloss:0.168004\n",
            "[220]\ttrain-mlogloss:0.091374\ttest-mlogloss:0.167358\n",
            "[240]\ttrain-mlogloss:0.085594\ttest-mlogloss:0.167841\n",
            "[260]\ttrain-mlogloss:0.080272\ttest-mlogloss:0.167558\n",
            "Stopping. Best iteration:\n",
            "[225]\ttrain-mlogloss:0.089916\ttest-mlogloss:0.167178\n",
            "\n",
            "[0]\ttrain-mlogloss:1.01036\ttest-mlogloss:1.01164\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.365499\ttest-mlogloss:0.38674\n",
            "[40]\ttrain-mlogloss:0.209025\ttest-mlogloss:0.239638\n",
            "[60]\ttrain-mlogloss:0.17215\ttest-mlogloss:0.209071\n",
            "[80]\ttrain-mlogloss:0.152413\ttest-mlogloss:0.195916\n",
            "[100]\ttrain-mlogloss:0.139291\ttest-mlogloss:0.191748\n",
            "[120]\ttrain-mlogloss:0.12812\ttest-mlogloss:0.187419\n",
            "[140]\ttrain-mlogloss:0.118676\ttest-mlogloss:0.185396\n",
            "[160]\ttrain-mlogloss:0.110581\ttest-mlogloss:0.183882\n",
            "[180]\ttrain-mlogloss:0.103154\ttest-mlogloss:0.183177\n",
            "[200]\ttrain-mlogloss:0.096321\ttest-mlogloss:0.182216\n",
            "[220]\ttrain-mlogloss:0.09014\ttest-mlogloss:0.18181\n",
            "[240]\ttrain-mlogloss:0.08434\ttest-mlogloss:0.181477\n",
            "[260]\ttrain-mlogloss:0.078953\ttest-mlogloss:0.181952\n",
            "Stopping. Best iteration:\n",
            "[223]\ttrain-mlogloss:0.089343\ttest-mlogloss:0.181407\n",
            "\n",
            "[0]\ttrain-mlogloss:0.976639\ttest-mlogloss:0.975432\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[20]\ttrain-mlogloss:0.35585\ttest-mlogloss:0.355539\n",
            "[40]\ttrain-mlogloss:0.218618\ttest-mlogloss:0.221647\n",
            "[60]\ttrain-mlogloss:0.173247\ttest-mlogloss:0.179535\n",
            "[80]\ttrain-mlogloss:0.1539\ttest-mlogloss:0.167149\n",
            "[100]\ttrain-mlogloss:0.140735\ttest-mlogloss:0.161919\n",
            "[120]\ttrain-mlogloss:0.129941\ttest-mlogloss:0.159239\n",
            "[140]\ttrain-mlogloss:0.120925\ttest-mlogloss:0.157237\n",
            "[160]\ttrain-mlogloss:0.112652\ttest-mlogloss:0.15677\n",
            "[180]\ttrain-mlogloss:0.105388\ttest-mlogloss:0.155676\n",
            "[200]\ttrain-mlogloss:0.098595\ttest-mlogloss:0.155386\n",
            "[220]\ttrain-mlogloss:0.092299\ttest-mlogloss:0.155283\n",
            "[240]\ttrain-mlogloss:0.086503\ttest-mlogloss:0.154214\n",
            "[260]\ttrain-mlogloss:0.081181\ttest-mlogloss:0.153608\n",
            "[280]\ttrain-mlogloss:0.07631\ttest-mlogloss:0.154153\n",
            "[300]\ttrain-mlogloss:0.071648\ttest-mlogloss:0.153837\n",
            "Stopping. Best iteration:\n",
            "[269]\ttrain-mlogloss:0.07888\ttest-mlogloss:0.153123\n",
            "\n",
            "cv scores :  [0.15455294325611724, 0.15581271337122118, 0.14844004090872331, 0.18052291337949633, 0.16031699039649816, 0.1797216791860578, 0.14480913016359687, 0.16717841054536114, 0.18140664982752264, 0.15312261741074518]\n",
            "0.16258840884453396\n",
            "totally time cost: 100m 44.95s\n"
          ]
        }
      ],
      "source": [
        "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=2017)\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 3])\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.4)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"cv scores : \", cv_scores)\n",
        "print(sum(cv_scores) / 10)\n",
        "pred_full_test = pred_full_test / 10.\n",
        "\n",
        "out_df = pd.DataFrame(pred_full_test)\n",
        "out_df.columns = ['EAP', 'HPL', 'MWS']\n",
        "out_df.insert(0, 'id', test_id)\n",
        "out_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/2210 Spooky Author Identification/sub.csv\", index=False)\n",
        "\n",
        "end_time = time()\n",
        "print('totally time cost: %dm %.2fs' % ((end_time-start_time)/60, (end_time-start_time)%60))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}